{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TPE INF4248 TAL/NLP\n",
        "## GROUPE 4\n",
        "### SUJET: STEMMATISATION"
      ],
      "metadata": {
        "id": "RCUR3B8xMXO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.   PorterStemmer\n",
        "\n",
        "### Description:\n",
        "C'est l'une des méthodes de radicalisation les plus populaires proposées en 1980. Elle est basée sur l'idée que les suffixes de la langue anglaise sont constitués d'une combinaison de suffixes plus petits et plus simples.\n",
        "Ce stemmer est connu pour sa rapidité et sa simplicité. Les principales applications de Porter Stemmer incluent l'exploration de données et la recherche d'informations. Cependant, ses applications ne sont limitées qu'aux mots anglais. De plus, le groupe de radicaux est mappé sur le même radical et le radical de sortie n'est pas nécessairement un mot significatif. Les algorithmes sont de nature assez longue et sont connus pour être les plus anciens stemmer.\n",
        "\n",
        "Exemple : EED -> EE signifie « si le mot a au moins une voyelle et une consonne plus la terminaison EED, changez la terminaison en EE » car « agreed » devient « agree ».\n",
        "\n",
        "### Avantages :\n",
        "Il produit le meilleur résultat par rapport aux autres stemmers et il a moins de taux d'erreur.\n",
        "\n",
        "### Inconvenients :\n",
        "- Les variantes morphologiques produites ne sont pas toujours de vrais mots.\n",
        "\n",
        "- PorterStemmer est l'un des meilleurs algorithmes de Stemmatisation."
      ],
      "metadata": {
        "id": "gcwoYbOgLuN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import *\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "5iWD1nVJLtT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Création d'une instance de PorterStemmer"
      ],
      "metadata": {
        "id": "RGme0vQdbXXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "-Btzc6B2bRs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialisation de nos données pour tester le Stemmer"
      ],
      "metadata": {
        "id": "p1EGLJwgbWGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "url = '/content/drive/My Drive/INF4248/Dataset/stemming/EmotionHappy.csv'\n",
        "\n",
        "df_happy = pd.read_csv(url)\n",
        "\n",
        "# Suppression des doublons\n",
        "df_happy = df_happy.drop_duplicates(subset=['content', 'sentiment'])\n",
        "df_happy = df_happy.reset_index(drop=True)\n",
        "\n",
        "print(df_happy.head())\n",
        "print('\\n')\n",
        "print(df_happy.info())\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aM4j5tj7cQLG",
        "outputId": "ae63028e-dfb8-448c-d162-077eea7a19ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "                                             content sentiment\n",
            "0  Wants to know how the hell I can remember word...     happy\n",
            "1  Love is a long sweet dream & marriage is an al...     happy\n",
            "2  The world could be amazing when you are slight...     happy\n",
            "3  My secret talent is getting tired without doin...     happy\n",
            "4  Khatarnaak Whatsapp Status Ever… Can\\’t talk, ...     happy\n",
            "\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 704 entries, 0 to 703\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   content    704 non-null    object\n",
            " 1   sentiment  704 non-null    object\n",
            "dtypes: object(2)\n",
            "memory usage: 11.1+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour nettoyer les données et créer des nuages ​​de mots, nous nous sommes inspiré du notebook suivant : https://www.kaggle.com/moezabid/disaster-tweets-nlp"
      ],
      "metadata": {
        "id": "tb5q-hx4vNxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from textblob import TextBlob\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "yawBI0UZvxw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contractions = {\n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"needn't\": \"need not\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who's\": \"who is\",\n",
        "\"won't\": \"will not\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you're\": \"you are\",\n",
        "\"thx\"   : \"thanks\"\n",
        "}\n",
        "\n",
        "\n",
        "def clean(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]','',text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+','',text)\n",
        "    text = re.sub('<,*?>+\"','',text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation),'',text)\n",
        "    text = re.sub('\\n','',text)\n",
        "    text = re.sub('\\w*\\d\\w*','',text)\n",
        "    text = re.sub(\"xa0'\", '', text)\n",
        "    text = re.sub(u\"\\U00002019\", \"'\", text) # IMPORTANT: le caractère apostrophe n'était pas le caractère habituel...\n",
        "    words = text.split()\n",
        "    for i in range(len(words)):\n",
        "        if words[i].lower() in contractions.keys():\n",
        "            words[i] = contractions[words[i].lower()]\n",
        "    text = \" \".join(words)\n",
        "    text = TextBlob(text).correct()\n",
        "    return text\n",
        "\n",
        "df_happy['content'] = df_happy['content'].apply(lambda x: clean(x))\n",
        "\n",
        "# Suppression des lignes de données vides\n",
        "df_happy['content'].replace('', np.nan, inplace=True)\n",
        "df_happy = df_happy.dropna(subset = ['content'])\n",
        "df_happy = df_happy.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "m1kuN-GUu6zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppresssion des stopwords"
      ],
      "metadata": {
        "id": "o_ImZvI_wVo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaGOKWNSwg4g",
        "outputId": "dae6257e-e215-417c-c936-2ff77a1e9bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    text = text.split()\n",
        "    words = [w for w in text if w not in stopwords.words('english')]\n",
        "    return \" \".join(words)\n",
        "\n",
        "df_happy['content_no_sw'] = df_happy['content'].apply(lambda x : remove_stopwords(x))"
      ],
      "metadata": {
        "id": "u_GEVuRpwY7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recupération du texte pour l'execution des Stemmer"
      ],
      "metadata": {
        "id": "4vcuMndtwtqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = df_happy['content'].values.copy()\n",
        "words_tk = []\n",
        "for sentence in sentences:\n",
        "        text = str(sentence)\n",
        "        words_tk.append(word_tokenize(text))\n",
        "\n",
        "words_for_stem = words_tk[0]\n",
        "print(words_for_stem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIoehXcXw4tl",
        "outputId": "052fc4dd-cc71-43c2-cb52-5d0ab1f5cfeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wants', 'to', 'know', 'how', 'the', 'hell', 'i', 'can', 'remember', 'words', 'to', 'songs', 'from', 'years', 'ago', 'but', 'can', 'not', 'remember', 'what', 'i', 'went', 'into', 'the', 'next', 'room', 'for']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execution de PorterStemmer sur nos données"
      ],
      "metadata": {
        "id": "EuKDqzL-cad_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words_for_stem:\n",
        "    print(word, \" : \", ps.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AInYOGvscfjj",
        "outputId": "0423bd22-5da9-48de-89fc-783579bff525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wants  :  want\n",
            "to  :  to\n",
            "know  :  know\n",
            "how  :  how\n",
            "the  :  the\n",
            "hell  :  hell\n",
            "i  :  i\n",
            "can  :  can\n",
            "remember  :  rememb\n",
            "words  :  word\n",
            "to  :  to\n",
            "songs  :  song\n",
            "from  :  from\n",
            "years  :  year\n",
            "ago  :  ago\n",
            "but  :  but\n",
            "can  :  can\n",
            "not  :  not\n",
            "remember  :  rememb\n",
            "what  :  what\n",
            "i  :  i\n",
            "went  :  went\n",
            "into  :  into\n",
            "the  :  the\n",
            "next  :  next\n",
            "room  :  room\n",
            "for  :  for\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Execution de PorterStemmer sur des mots où nous obtenons du sur-racinement (over-stemming) qui est une erreur de radicalisation, faisant référence à la situation où un stemmer produit une forme racine qui n'est pas un mot valide ou qui n'est pas la forme racine correcte d'un mot."
      ],
      "metadata": {
        "id": "HAgoRDPMA1C7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"language\", \"history\", \"historical\", \"article\", \"programmers\", \"poodle\", \"eaten\", \"mouse\", \"remember\", \"update\", \"easily\", \"leaves\", \"university\", \"fairly\", \"sportingly\"]\n",
        "\n",
        "for word in words:\n",
        "  print(word, \":\" , ps.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hk-JgUHJBJT2",
        "outputId": "d18276c5-624d-400e-b90c-b796e653cd1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "language : languag\n",
            "history : histori\n",
            "historical : histor\n",
            "article : articl\n",
            "programmers : programm\n",
            "poodle : poodl\n",
            "eaten : eaten\n",
            "mouse : mous\n",
            "remember : rememb\n",
            "update : updat\n",
            "easily : easili\n",
            "leaves : leav\n",
            "university : univers\n",
            "fairly : fairli\n",
            "sportingly : sportingli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. SnowBallStemmer\n",
        "\n",
        "### Description:\n",
        "Il s'agit d'un algorithme de stemming également connu sous le nom d'algorithme de stemming Porter2 car il s'agit d'une meilleure version du PorterStemmer puisque certains problèmes ont été résolus dans ce stemmer.\n",
        "\n",
        "Example: sitting -> sitt -> sit\n",
        "\n",
        "### Avantages :\n",
        "Prise en charge de plusieurs langues\n",
        "Elle est très rapide et peut gérer le retrait des lettres doubles dans des mots comme \"getting\" étant transformé en \"get\" et gère également les pluriels irréguliers comme 'teeth' et 'tooth' etc.\n",
        "\n",
        "### Inconvenients :\n",
        "Elle prend du temps et de nombreux suffixes ne sont pas disponibles dans le tableau des terminaisons. C'est très peu fiable."
      ],
      "metadata": {
        "id": "4oeseNGZDKXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cet algorithme prend en charge plusieurs langues\n",
        "print(\" \".join(SnowballStemmer.languages))"
      ],
      "metadata": {
        "id": "P6qBM09BDlt4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bfe7bc0-76d7-4ebc-c461-29ec147cec70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Différence entre Porter Stemmer et Snowball Stemmer :\n",
        "\n",
        "- Snowball Stemmer est plus agressif que Porter Stemmer.\n",
        "- Prend en charge plusieurs langues.\n",
        "- Il n'y a qu'une petite différence dans le fonctionnement de ces deux.\n",
        "Des mots comme \"fairly\" et \"sportingly\" ont été réduits en « fair » et « sport » dans snowball Stemmer, mais lorsque vous utilisez Porter Stemmer ils sont réduits en « fairli » et « sportingli ».\n",
        "La différence entre les deux algorithmes peut être clairement vue dans la façon dont le mot \"sportingly\" est contenu par les deux. Il est clair que Snowball Stemmer le ramène à une racine plus précise."
      ],
      "metadata": {
        "id": "M3e6WobmP69p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execution de LancasterStemmer sur notre liste de mots contenu dans words"
      ],
      "metadata": {
        "id": "zn-vLU7dTh8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "snow = SnowballStemmer(language='english')\n",
        "for word in words:\n",
        "    print(word, \":\", snow.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZhB0_OLQ1g6",
        "outputId": "c1049aac-953c-47b2-b29e-318952263263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "language : languag\n",
            "history : histori\n",
            "historical : histor\n",
            "article : articl\n",
            "programmers : programm\n",
            "poodle : poodl\n",
            "eaten : eaten\n",
            "mouse : mous\n",
            "remember : rememb\n",
            "update : updat\n",
            "easily : easili\n",
            "leaves : leav\n",
            "university : univers\n",
            "fairly : fair\n",
            "sportingly : sport\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Lancaster Stemmer\n",
        "\n",
        "### Description:\n",
        "C'est un algorithme itératif. Un tableau contenant environ cent vingt règles indexées par la dernière lettre d'un suffixe. Chaque règle spécifie soit la suppression soit le remplacement d'un suffixe. C'est simple et chaque itération prend soin des deux (suppression et remplacement) selon la règle appliquée.\n",
        "\n",
        "### Avantages :\n",
        "Simple et chaque itération prise en charge à la fois la suppression et le remplacement selon la règle appliquée.\n",
        "\n",
        "### Inconvenients :\n",
        "C'est un algorithme très lourd et le sur-racinement peut se produire.\n"
      ],
      "metadata": {
        "id": "rCJekqHySGYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lancas = LancasterStemmer()"
      ],
      "metadata": {
        "id": "jqtdazY_XHFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execution de LancasterStemmer sur notre liste de mots contenu dans words"
      ],
      "metadata": {
        "id": "7tTEJztjXAEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word, \":\", lancas.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZNylS4xXWnd",
        "outputId": "9820f8d9-7902-4639-9e91-dc4c5e37c1fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "language : langu\n",
            "history : hist\n",
            "historical : hist\n",
            "article : artic\n",
            "programmers : program\n",
            "poodle : poodl\n",
            "eaten : eat\n",
            "mouse : mous\n",
            "remember : rememb\n",
            "update : upd\n",
            "easily : easy\n",
            "leaves : leav\n",
            "university : univers\n",
            "fairly : fair\n",
            "sportingly : sport\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remarque:** Lancaster contrairement au deux autres (Porter et Snowball) à bien radicalisé \"programmers\" en \"program\", \"easily\" en \"easy\" mais à pour la majorité du reste Lancaster a aussi mal radicalisé."
      ],
      "metadata": {
        "id": "iAkujgt8XlDk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Regexp Stemmer\n",
        "\n",
        "### Description:\n",
        "\n",
        "Regex stemmer identifie les affixes morphologiques à l'aide d'expressions régulières. Les sous-chaînes correspondant aux expressions régulières seront ignorées.\n",
        "\n",
        "On lui donne en parametre une ou plusieurs suffixe(s) qu'il supprimera dans à la fin de chaque mots qu'on lui passera et la taille minimal (min) qu'un mot doit avoir pour être radicalisé.\n",
        "\n",
        "### Avantages:\n",
        "On définit nous même les terminaisons ainsi que la taille des mots à radicaliser.\n",
        "\n",
        "### Inconvenients:\n",
        "Ne peut vraiment être appliqué à un très grand volume de nom varié;\n",
        "Les mots obtenus ne seront forcément pas valide.\n"
      ],
      "metadata": {
        "id": "7JF58fnyYHen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regexp = RegexpStemmer('en$|s$|e$|able$|es$', min=4)"
      ],
      "metadata": {
        "id": "CktYZUlCa2B4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execution de Regexp Stemmer sur notre liste de mots contenu dans words"
      ],
      "metadata": {
        "id": "ktm-cKKpb3su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word, \":\", regexp.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4vA3NPUcHFJ",
        "outputId": "e44dd2fb-b207-477f-f1c7-b41d56a1a698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "language : languag\n",
            "history : history\n",
            "historical : historical\n",
            "article : articl\n",
            "programmers : programmer\n",
            "poodle : poodl\n",
            "eaten : eat\n",
            "mouse : mous\n",
            "remember : remember\n",
            "update : updat\n",
            "easily : easily\n",
            "leaves : leav\n",
            "university : university\n",
            "fairly : fairly\n",
            "sportingly : sportingly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Références:\n",
        " * https://medium.com/@ajay_khanna/different-techniques-of-stemming-a99d2fe8c08c [consulté le 26/03/2023 à 15h03]\n",
        " * https://www.ijrte.org/wp-content/uploads/papers/v8i4/C6200098319.pdf\n",
        "[consulté le 26/03/2023 à 18h16]\n",
        " * https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-stemming-in-natural-language-processing/ [consulté le 28/03/2023 à 04h02]\n",
        " * https://www.geeksforgeeks.org/introduction-to-stemming/ [consulté le 30/03/2023 à 05h05]\n",
        " * https://www.kaggle.com/moezabid/disaster-tweets-nlp [consulté le 30/03/2023 à 06h09]\n",
        "\n",
        " Dataset:\n",
        " - (Emotion Happy) https://www.kaggle.com/code/ronikdedhia/emotion-whatsapp-chat/input/?select=Emotion%28happy%29.csv [téléchargé le 30/03/2023 04h25]\n",
        " - Crée à partir de données provenant de certains des sites présents en référence.\n"
      ],
      "metadata": {
        "id": "HPSW3wsGfxLv"
      }
    }
  ]
}